{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import time as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tsfel\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# groq\n",
    "from groq import Groq\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY_1\"), # stored API key in virtual environment (not going to GitHub)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree on TSFEL features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MakeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n",
      "Training labels shape:  (126,)\n",
      "Testing labels shape:  (54,)\n"
     ]
    }
   ],
   "source": [
    "time = 10 # Setting the time window for each sample\n",
    "offset = 100 # Skipping the first 100 rows to remove noise\n",
    "folders = [\"LAYING\",\"SITTING\",\"STANDING\",\"WALKING\",\"WALKING_DOWNSTAIRS\",\"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=seed,stratify=y)\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape) # (number of samples, number of time steps (x (=10) sec * 50Hz), number of features)\n",
    "print(\"Testing data shape: \", X_test.shape)\n",
    "print(\"Training labels shape: \", y_train.shape)\n",
    "print(\"Testing labels shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSFEL feature extraction and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of of pandas dataframes corresponding to each sample\n",
    "X_train_dfs = [pd.DataFrame(sample, columns=['accx', 'accy', 'accz']) for sample in X_train]\n",
    "X_test_dfs = [pd.DataFrame(sample, columns=['accx', 'accy', 'accz']) for sample in X_test]\n",
    "\n",
    "X_train_dfs = [df.apply(lambda x: np.sqrt(x['accx']**2 + x['accy']**2 + x['accz']**2), axis=1) for df in X_train_dfs]\n",
    "X_test_dfs = [df.apply(lambda x: np.sqrt(x['accx']**2 + x['accy']**2 + x['accz']**2), axis=1) for df in X_test_dfs]\n",
    "\n",
    "# consider all features\n",
    "cfg_file = tsfel.get_features_by_domain()  \n",
    "\n",
    "# get list of feature vectors for each dataframe (or sample)           \n",
    "# choosing `fs=50` because the data was sampled at 50Hz                              \n",
    "X_train_tsfel_dfs = [tsfel.time_series_features_extractor(cfg_file, df, fs=50) for df in X_train_dfs]\n",
    "X_train_tsfel = pd.concat(X_train_tsfel_dfs, axis=0).fillna(0).values\n",
    "\n",
    "X_test_tsfel_dfs = [tsfel.time_series_features_extractor(cfg_file, df, fs=50) for df in X_test_dfs]\n",
    "X_test_tsfel = pd.concat(X_test_tsfel_dfs, axis=0).fillna(0).values\n",
    "\n",
    "# we leave out removing constant columns and highly correlated features as the custom data may not have the same features as the provided dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSFEL Accuracy: 0.6666666666666666\n",
      "TSFEL Precision: 0.6554843304843304\n",
      "TSFEL Recall: 0.6666666666666666\n",
      "\n",
      "TSFEL Confusion Matrix:\n",
      "[[7 0 1 0 1 0]\n",
      " [2 4 2 0 1 0]\n",
      " [0 2 7 0 0 0]\n",
      " [0 0 0 2 1 6]\n",
      " [0 0 0 0 9 0]\n",
      " [0 0 0 2 0 7]]\n"
     ]
    }
   ],
   "source": [
    "dtc_tsfel = DecisionTreeClassifier()\n",
    "dtc_tsfel.fit(X_train_tsfel, y_train)\n",
    "y_pred_tsfel = dtc_tsfel.predict(X_test_tsfel)\n",
    "acc_tsfel = accuracy_score(y_test, y_pred_tsfel)\n",
    "\n",
    "# Since, this model is using the provided dataset which is balanced, we will use macro average for precision and recall\n",
    "prec_tsfel = precision_score(y_test, y_pred_tsfel, average='macro')\n",
    "rec_tsfel = recall_score(y_test, y_pred_tsfel, average='macro')\n",
    "\n",
    "conf_mx_tsfel = confusion_matrix(y_test, y_pred_tsfel)\n",
    "\n",
    "print(\"TSFEL Accuracy:\", acc_tsfel)\n",
    "print(\"TSFEL Precision:\", prec_tsfel)\n",
    "print(\"TSFEL Recall:\", rec_tsfel)\n",
    "print()\n",
    "print(\"TSFEL Confusion Matrix:\")\n",
    "print(conf_mx_tsfel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree on Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 125\n",
    "\n",
    "custom_data_dir = os.path.join(\"Krustykrabs\")\n",
    "\n",
    "X_custom_data=[]\n",
    "y_custom_data=[]\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(custom_data_dir,folder))\n",
    "\n",
    "    for file in files:\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(os.path.join(custom_data_dir,folder,file),sep=\",\",header=0)\n",
    "        df.drop([\"time\", \"TgF\"], axis=1, inplace=True)\n",
    "\n",
    "        df = df[offset:offset+time*50]\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = (2 * (df[col] - df[col].min()) / (df[col].max() - df[col].min())) - 1\n",
    "\n",
    "        X_custom_data.append(df.values)\n",
    "        y_custom_data.append(classes[folder])\n",
    "\n",
    "\n",
    "seed = 4\n",
    "X_train_custom_data, X_test_custom_data, y_train_custom_data, y_test_custom_data = train_test_split(X_custom_data,y_custom_data,test_size=0.3,random_state=seed,stratify=y_custom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dfs_custom_data = [pd.DataFrame(sample, columns=['accx', 'accy', 'accz']) for sample in X_train_custom_data]\n",
    "X_test_dfs_custom_data = [pd.DataFrame(sample, columns=['accx', 'accy', 'accz']) for sample in X_test_custom_data]\n",
    "\n",
    "X_train_dfs_custom_data = [df.apply(lambda x: np.sqrt(x['accx']**2 + x['accy']**2 + x['accz']**2), axis=1) for df in X_train_dfs_custom_data]\n",
    "X_test_dfs_custom_data = [df.apply(lambda x: np.sqrt(x['accx']**2 + x['accy']**2 + x['accz']**2), axis=1) for df in X_test_dfs_custom_data]\n",
    "\n",
    "tsfel_config_custom_data = tsfel.get_features_by_domain()\n",
    "\n",
    "X_train_tsfel_dfs_custom_data = [tsfel.time_series_features_extractor(tsfel_config_custom_data, df, fs=50) for df in X_train_dfs_custom_data]\n",
    "X_train_tsfel_custom_data = pd.concat(X_train_tsfel_dfs_custom_data, axis=0).fillna(0).values\n",
    "\n",
    "X_test_tsfel_dfs_custom_data = [tsfel.time_series_features_extractor(tsfel_config_custom_data, df, fs=50) for df in X_test_dfs_custom_data]\n",
    "X_test_tsfel_custom_data = pd.concat(X_test_tsfel_dfs_custom_data, axis=0).fillna(0).values\n",
    "\n",
    "# we leave out removing constant columns and highly correlated features as the custom data may not have the same features as the provided dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previously Trained Decision Tree on Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSFEL Accuracy: 0.2222222222222222\n",
      "TSFEL Precision: 0.047619047619047616\n",
      "TSFEL Recall: 0.16666666666666666\n",
      "\n",
      "TSFEL Confusion Matrix:\n",
      "[[0 0 2 0 0 0]\n",
      " [0 0 2 0 0 0]\n",
      " [0 0 2 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devanshlodha/Documents/IIT Gandhinagar/Courses/ES335_Machine_Learning/es335-24-fall-assignment-1/venv_es335_1/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_tsfel_custom_data = dtc_tsfel.predict(X_test_tsfel_custom_data)\n",
    "acc_tsfel_custom_data = accuracy_score(y_test_custom_data, y_pred_tsfel_custom_data)\n",
    "\n",
    "# Since, this model is using the provided dataset which is balanced, we will use macro average for precision and recall\n",
    "prec_tsfel_custom_data = precision_score(y_test_custom_data, y_pred_tsfel_custom_data, average='macro')\n",
    "rec_tsfel_custom_data = recall_score(y_test_custom_data, y_pred_tsfel_custom_data, average='macro')\n",
    "\n",
    "conf_mx_tsfel_custom_data = confusion_matrix(y_test_custom_data, y_pred_tsfel_custom_data)\n",
    "\n",
    "print(\"TSFEL Accuracy:\", acc_tsfel_custom_data)\n",
    "print(\"TSFEL Precision:\", prec_tsfel_custom_data)\n",
    "print(\"TSFEL Recall:\", rec_tsfel_custom_data)\n",
    "print()\n",
    "print(\"TSFEL Confusion Matrix:\")\n",
    "print(conf_mx_tsfel_custom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensor signals from the UCI-HAR were pre-processed by applying noise filters. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. But for our data we are not using any sort of filtering. \n",
    "\n",
    "Also, the walking speed of people in the west is generally higher than us. The model trained on the UCI-HAR dataset may not work well on our data because of such differences in gait speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feautrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "time = 10\n",
    "offset = 125\n",
    "\n",
    "custom_data_dir = os.path.join(\"Krustykrabs\")\n",
    "\n",
    "subject_name = {1: \"Tejas\", 2: \"Devansh\", 3: \"Mohit\", 4: \"Devansh\", 5: \"Mohit\"}\n",
    "\n",
    "cfg_file = tsfel.get_features_by_domain() \n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(custom_data_dir,folder))\n",
    "\n",
    "    for file in files:\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(custom_data_dir,folder,file),sep=\",\",header=0)\n",
    "        df.drop([\"time\", \"TgF\"], axis=1, inplace=True)\n",
    "        df = df[offset:offset+time*50]\n",
    "        for col in df.columns:\n",
    "            df[col] = (2 * (df[col] - df[col].min()) / (df[col].max() - df[col].min())) - 1\n",
    "        df=tsfel.time_series_features_extractor(cfg_file, df, fs=50)\n",
    "        df[\"subject_id\"] = subject_name[int(file[8])]\n",
    "        df[\"activity\"] = classes[folder]\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# remove columns where the feature is constant throughout all samples\n",
    "for col in data.columns:\n",
    "    if len(data[col].unique()) == 1:\n",
    "        data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# remove highly correlated features (columns)\n",
    "corr = data.iloc[:, [data.columns.get_loc(col) for col in data.columns if col not in ['subject_id', 'activity']]].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "tri_df = corr.mask(mask)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.9)] # threshold = 0.9\n",
    "data.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# shuffle data\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Subject Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Devansh', 'Mohit', 'Tejas'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subjects = data['subject_id'].unique()\n",
    "all_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty list to store accuracy in  \n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all subject IDs in the all_subjects list and follow this procedure:\n",
    "\n",
    "- For each iteration, select only the observations containing the subject ID as the test dataset, and select all other observations as the training set. The labels for the test/train split are selected in the same way. For the data, the subject_ID, and activity must be dropped.\n",
    "- Fit the model and calculate the accuracy.\n",
    "- Store the accuracy in the empty list created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.6666666666666666\n",
      "0.5\n",
      "Average Accuracy:  0.4290123456790123\n"
     ]
    }
   ],
   "source": [
    "#loop over all conditions  \n",
    "for idx, subject_id in enumerate(all_subjects):\n",
    "    dtc_losocv = DecisionTreeClassifier()\n",
    "\n",
    "    # assign testing and training data  \n",
    "    x_train_losocv = data.loc[data['subject_id'] != subject_id].drop(['subject_id', 'activity'], axis=1)\n",
    "    x_test_losocv = data.loc[data['subject_id'] == subject_id].drop(['subject_id', 'activity'], axis=1)\n",
    "    y_train_losocv = data.loc[data['subject_id'] != subject_id]['activity']\n",
    "    y_test_losocv = data.loc[data['subject_id'] == subject_id]['activity']\n",
    "      \n",
    "    # fit model \n",
    "    dtc_losocv.fit(x_train_losocv, y_train_losocv)  \n",
    "    y_pred_losocv = dtc_losocv.predict(x_test_losocv)\n",
    "    # append max accuracy over all epochs  \n",
    "    accuracy_list.append(accuracy_score(y_test_losocv, y_pred_losocv))\n",
    "    print(accuracy_list[-1])\n",
    "      \n",
    "# get average of accuracies over all cross folds  \n",
    "mean_accuracy = np.mean(np.array(accuracy_list)) \n",
    "\n",
    "print('Average Accuracy: ', mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126, 384), (54, 384), (21, 384), (9, 384))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tsfel.shape, X_test_tsfel.shape, X_train_tsfel_custom_data.shape, X_test_tsfel_custom_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 54, 21, 9)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), len(y_test), len(y_train_custom_data), len(y_test_custom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "Examples are taken from UCI-HAR and the LLM is asked to predict on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {1: \"WALKING\", 2: \"WALKING_UPSTAIRS\", 3: \"WALKING_DOWNSTAIRS\", 4: \"SITTING\", 5: \"STANDING\", 6: \"LAYING\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '4: SITTING')\n",
      "(1, '3: WALKING_DOWNSTAIRS')\n",
      "(2, '1: WALKING')\n",
      "(3, '1: WALKING')\n",
      "(4, '1: WALKING')\n",
      "(5, '4: SITTING')\n",
      "(6, '1: WALKING')\n",
      "(7, '1: WALKING')\n",
      "(8, '1: WALKING')\n"
     ]
    }
   ],
   "source": [
    "y_few_shot_pred_tup = []\n",
    "for i in range(9):\n",
    "    query = f\"*Your task is to classify the activity performed by the user based on the provided featurized accelerometer data. \\n* You will be given a TSFEL feature vector of size 384. \\n* There are six possible activities - 1: WALKING, 2: WALKING_UPSTAIRS, 3: WALKING_DOWNSTAIRS, 4: SITTING, 5: STANDING, 6: LAYING.\\n* Please provide the most likely activity as a single integer corresponding to the activity.\\n Here are few examples with feature vectors and what activity the correspond to:\"\n",
    "    # giving it random 10 examples from the UCI-HAR training data\n",
    "    for j in np.random.randint(0, 126, 10):\n",
    "        query+=f\"{j+1}.\\n\"\n",
    "        query+=f\"Feature vector = {[float(x) for x in X_train_tsfel[j]]}\\n\"\n",
    "        query+=f\"Activity = {y_train[j]}: {label[y_train[j]]}\\n\"\n",
    "    # asking about an example from the custom data\n",
    "    query+=f\"\\nWhat is this activity: {[float(x) for x in X_test_tsfel_custom_data[i]]}?\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            # Set an optional system message. This sets the behavior of the\n",
    "            # assistant and can be used to provide specific instructions for\n",
    "            # how it should behave throughout the conversation.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are an activity classification model. You'll be given a TSFEL feature vector of size 384. Keep responses in the following format: 1: WALKING, 2: WALKING_UPSTAIRS, 3: WALKING_DOWNSTAIRS, 4: SITTING, 5: STANDING, 6: LAYING. You should output a single integer corresponding to the activity label.\"\n",
    "            },\n",
    "            # Set a user message for the assistant to respond to.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        # The language model which will generate the completion.\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        # optional parameters\n",
    "\n",
    "        # Controls randomness: lowering results in less random completions.\n",
    "        temperature=0,\n",
    "    )\n",
    "    T.sleep(2)\n",
    "    # append the completion returned by the LLM to y_pred\n",
    "    y_few_shot_pred_tup.append((i,chat_completion.choices[0].message.content))\n",
    "    print(y_few_shot_pred_tup[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_few_shot_pred = [int(x[1][0]) for x in y_few_shot_pred_tup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_few_shot_pred, y_test_custom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very bad performance since this is an LLM model and our data is very different from the UCI-HAR dataset. Also we're not telling the LLM the names of the feature so as to give it an hint of what the numbers mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_tsfel_dfs_custom_data[0].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the list of dataframes into a single dataframe\n",
    "col = X_train_tsfel_dfs_custom_data[0].columns\n",
    "X_train_tsfel_df_custom_data = pd.DataFrame(X_train_tsfel_custom_data, columns = col)\n",
    "X_test_tsfel_df_custom_data = pd.DataFrame(X_test_tsfel_custom_data, columns = col)\n",
    "\n",
    "# do the following for the training data and then choose remaining columns from the test data\n",
    "# remove columns where the feature is constant throughout all samples\n",
    "for col in X_train_tsfel_df_custom_data.columns:\n",
    "    if len(X_train_tsfel_df_custom_data[col].unique()) == 1:\n",
    "        X_train_tsfel_df_custom_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# remove highly correlated features (columns) from the training data\n",
    "corr = X_train_tsfel_df_custom_data.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "tri_df = corr.mask(mask)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.9)] # threshold = 0.9\n",
    "X_train_tsfel_df_custom_data.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# remove the same columns from the test data\n",
    "X_test_tsfel_df_custom_data = X_test_tsfel_df_custom_data[X_train_tsfel_df_custom_data.columns] \n",
    "\n",
    "# Filter columns starting with '0_FFT mean coefficient_'\n",
    "filtered_cols = [col for col in X_train_tsfel_df_custom_data.columns if col.startswith('0_FFT mean coefficient_')]\n",
    "# Calculate the sum of squares for the filtered columns to get energy\n",
    "X_train_tsfel_df_custom_data['0_Energy'] = X_train_tsfel_df_custom_data[filtered_cols].pow(2).sum(axis=1)\n",
    "X_test_tsfel_df_custom_data['0_Energy'] = X_test_tsfel_df_custom_data[filtered_cols].pow(2).sum(axis=1)\n",
    "# Drop the filtered columns\n",
    "X_train_tsfel_df_custom_data.drop(filtered_cols, axis=1, inplace=True)\n",
    "X_test_tsfel_df_custom_data.drop(filtered_cols, axis=1, inplace=True)\n",
    "\n",
    "# feature selection\n",
    "features = ['0_Entropy', '0_Fundamental frequency',\n",
    "       '0_Human range energy', '0_Kurtosis', '0_Max',\n",
    "       '0_Max power spectrum', '0_Mean diff', '0_Median diff', '0_Min',\n",
    "       '0_Neighbourhood peaks', '0_Peak to peak distance',\n",
    "       '0_Positive turning points', '0_Power bandwidth', '0_Root mean square',\n",
    "       '0_Skewness', '0_Slope', '0_Spectral distance',\n",
    "       '0_Spectral entropy', '0_Spectral positive turning points',\n",
    "       '0_Wavelet absolute mean_8', '0_Wavelet entropy', '0_Wavelet variance_8', '0_Energy']\n",
    "\n",
    "X_train_tsfel_df_custom_data = X_train_tsfel_df_custom_data[features]\n",
    "X_test_tsfel_df_custom_data = X_test_tsfel_df_custom_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "Examples are taken from our data and the LLM is asked to predict on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 23), (9, 23), 21, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tsfel_df_custom_data.shape, X_test_tsfel_df_custom_data.shape, len(y_train_custom_data), len(y_test_custom_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '1: WALKING')\n",
      "(1, '3: WALKING_DOWNSTAIRS')\n",
      "(2, '4: SITTING')\n",
      "(3, '1: WALKING')\n",
      "(4, '4: SITTING')\n",
      "(5, '1: WALKING')\n",
      "(6, '1: WALKING')\n",
      "(7, '1: WALKING')\n",
      "(8, '1: WALKING')\n"
     ]
    }
   ],
   "source": [
    "y_few_shot_pred_tup = []\n",
    "for i in range(9):\n",
    "    query = f\"*Your task is to classify the activity performed by the user based on the provided featurized accelerometer data.\\n* The features are: {features}. \\n* You are given values corresponding to the features in order. \\n* There are six possible activities - 1: WALKING, 2: WALKING_UPSTAIRS, 3: WALKING_DOWNSTAIRS, 4: SITTING, 5: STANDING, 6: LAYING.\\n* Please provide the most likely activity as a single integer corresponding to the activity.\\n Here are few examples with values corresponding to the given features:\"\n",
    "    # giving it random 10 examples from the training data\n",
    "    for j in np.random.randint(0, 21, 10):\n",
    "        query+=f\"{j+1}.\\n\"\n",
    "        query+=f\"Feature vector = {list(X_train_tsfel_df_custom_data.loc[j].to_dict().values())}\\n\"\n",
    "        query+=f\"Activity = {y_train[j]}: {label[y_train[j]]}\\n\"\n",
    "    query+=f\"\\nWhat is this activity: {list(X_test_tsfel_df_custom_data.loc[i].to_dict().values())}?\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            # Set an optional system message. This sets the behavior of the\n",
    "            # assistant and can be used to provide specific instructions for\n",
    "            # how it should behave throughout the conversation.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are an activity classification model. Keep responses in the following format: 1: WALKING, 2: WALKING_UPSTAIRS, 3: WALKING_DOWNSTAIRS, 4: SITTING, 5: STANDING, 6: LAYING. You should output a single integer corresponding to the activity label.\"\n",
    "            },\n",
    "            # Set a user message for the assistant to respond to.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        # The language model which will generate the completion.\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        # optional parameters\n",
    "\n",
    "        # Controls randomness: lowering results in less random completions.\n",
    "        temperature=0,\n",
    "    )\n",
    "    T.sleep(2)\n",
    "    # append the completion returned by the LLM to y_pred\n",
    "    y_few_shot_pred_tup.append((i,chat_completion.choices[0].message.content))\n",
    "    print(y_few_shot_pred_tup[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_few_shot_pred = [int(x[1][0]) for x in y_few_shot_pred_tup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 1, 4, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_few_shot_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_few_shot_pred, y_test_custom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance is better than random."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_es335_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
